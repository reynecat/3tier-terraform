"""
EKS Auto Recovery Lambda Function
ì•ŒëŒ ë°œìƒ ì‹œ ìë™ ë³µêµ¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import json
import boto3
import os
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS í´ë¼ì´ì–¸íŠ¸
ec2 = boto3.client('ec2')
eks = boto3.client('eks')
autoscaling = boto3.client('autoscaling')
sns = boto3.client('sns')

# í™˜ê²½ ë³€ìˆ˜
CLUSTER_NAME = os.environ.get('CLUSTER_NAME', '')
SNS_TOPIC_ARN = os.environ.get('SNS_TOPIC_ARN', '')
ENVIRONMENT = os.environ.get('ENVIRONMENT', 'prod')


def handler(event, context):
    """
    ë©”ì¸ Lambda í•¸ë“¤ëŸ¬
    SNSë¡œë¶€í„° CloudWatch ì•ŒëŒ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    logger.info(f"Received event: {json.dumps(event)}")

    try:
        # SNS ë©”ì‹œì§€ íŒŒì‹±
        for record in event.get('Records', []):
            message = json.loads(record['Sns']['Message'])
            alarm_name = message.get('AlarmName', '')
            new_state = message.get('NewStateValue', '')

            logger.info(f"Processing alarm: {alarm_name}, State: {new_state}")

            # ALARM ìƒíƒœì¼ ë•Œë§Œ ë³µêµ¬ ì‘ì—… ìˆ˜í–‰
            if new_state == 'ALARM':
                recovery_result = perform_recovery(alarm_name, message)
                send_notification(alarm_name, recovery_result)
            else:
                logger.info(f"Alarm {alarm_name} is in {new_state} state. No action needed.")

        return {
            'statusCode': 200,
            'body': json.dumps('Recovery check completed')
        }

    except Exception as e:
        logger.error(f"Error processing event: {str(e)}")
        raise


def perform_recovery(alarm_name: str, alarm_data: dict) -> dict:
    """
    ì•ŒëŒ ìœ í˜•ì— ë”°ë¥¸ ë³µêµ¬ ì‘ì—… ìˆ˜í–‰
    """
    result = {
        'alarm_name': alarm_name,
        'action_taken': 'none',
        'success': True,
        'details': ''
    }

    try:
        # ë…¸ë“œ ìƒíƒœ ì²´í¬ ì‹¤íŒ¨ ì•ŒëŒ
        if 'status-check-failed' in alarm_name.lower():
            result = handle_node_status_check_failed(alarm_data)

        # Pod ì¬ì‹œì‘ íšŸìˆ˜ ì´ˆê³¼ ì•ŒëŒ
        elif 'pod-restart' in alarm_name.lower():
            result = handle_pod_restart_high(alarm_data)

        # ë…¸ë“œ ìˆ˜ ë¶€ì¡± ì•ŒëŒ
        elif 'node-count-low' in alarm_name.lower():
            result = handle_node_count_low(alarm_data)

        # CPU/ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ì•ŒëŒ
        elif 'cpu-high' in alarm_name.lower() or 'memory-high' in alarm_name.lower():
            result = handle_resource_pressure(alarm_data)

        # Unhealthy í˜¸ìŠ¤íŠ¸ ì•ŒëŒ
        elif 'unhealthy-hosts' in alarm_name.lower():
            result = handle_unhealthy_hosts(alarm_data)

        else:
            result['details'] = f"No auto-recovery action defined for alarm: {alarm_name}"
            logger.info(result['details'])

    except Exception as e:
        result['success'] = False
        result['details'] = f"Recovery failed: {str(e)}"
        logger.error(result['details'])

    return result


def handle_node_status_check_failed(alarm_data: dict) -> dict:
    """
    ë…¸ë“œ ìƒíƒœ ì²´í¬ ì‹¤íŒ¨ ì‹œ ë³µêµ¬
    - ë¹„ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¢…ë£Œí•˜ê³  ASGê°€ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹œì‘í•˜ë„ë¡ í•¨
    """
    result = {
        'alarm_name': alarm_data.get('AlarmName', ''),
        'action_taken': 'terminate_unhealthy_instance',
        'success': True,
        'details': ''
    }

    try:
        # EKS ë…¸ë“œ ê·¸ë£¹ì˜ ASG ì°¾ê¸°
        nodegroups = eks.list_nodegroups(clusterName=CLUSTER_NAME)

        for ng_name in nodegroups.get('nodegroups', []):
            ng_info = eks.describe_nodegroup(
                clusterName=CLUSTER_NAME,
                nodegroupName=ng_name
            )

            # ASG ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            asg_name = ng_info['nodegroup']['resources']['autoScalingGroups'][0]['name']

            # ASG ë‚´ ë¹„ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸°
            asg_response = autoscaling.describe_auto_scaling_groups(
                AutoScalingGroupNames=[asg_name]
            )

            for asg in asg_response.get('AutoScalingGroups', []):
                for instance in asg.get('Instances', []):
                    instance_id = instance['InstanceId']

                    # EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸
                    ec2_status = ec2.describe_instance_status(
                        InstanceIds=[instance_id]
                    )

                    for status in ec2_status.get('InstanceStatuses', []):
                        instance_status = status.get('InstanceStatus', {}).get('Status', '')
                        system_status = status.get('SystemStatus', {}).get('Status', '')

                        if instance_status != 'ok' or system_status != 'ok':
                            logger.info(f"Terminating unhealthy instance: {instance_id}")

                            # ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ (ASGê°€ ìë™ìœ¼ë¡œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘)
                            autoscaling.terminate_instance_in_auto_scaling_group(
                                InstanceId=instance_id,
                                ShouldDecrementDesiredCapacity=False
                            )

                            result['details'] += f"Terminated unhealthy instance: {instance_id}. "

        if not result['details']:
            result['details'] = "No unhealthy instances found to terminate."

    except Exception as e:
        result['success'] = False
        result['details'] = f"Failed to handle status check failure: {str(e)}"

    return result


def handle_pod_restart_high(alarm_data: dict) -> dict:
    """
    Pod ì¬ì‹œì‘ íšŸìˆ˜ê°€ ë†’ì„ ë•Œì˜ ë³µêµ¬
    - ì•Œë¦¼ë§Œ ë³´ë‚´ê³  ìˆ˜ë™ ì¡°ì¹˜ ê¶Œì¥ (Pod ì¬ì‹œì‘ì€ k8sê°€ ìë™ ì²˜ë¦¬)
    """
    result = {
        'alarm_name': alarm_data.get('AlarmName', ''),
        'action_taken': 'notification_only',
        'success': True,
        'details': 'High pod restart count detected. Kubernetes will handle pod recovery automatically. '
                   'Manual investigation recommended to identify root cause.'
    }

    logger.info(result['details'])
    return result


def handle_node_count_low(alarm_data: dict) -> dict:
    """
    ë…¸ë“œ ìˆ˜ê°€ ìµœì†Œê°’ ë¯¸ë§Œì¼ ë•Œì˜ ë³µêµ¬
    - ASG desired capacity ì¦ê°€
    """
    result = {
        'alarm_name': alarm_data.get('AlarmName', ''),
        'action_taken': 'scale_up_nodes',
        'success': True,
        'details': ''
    }

    try:
        nodegroups = eks.list_nodegroups(clusterName=CLUSTER_NAME)

        for ng_name in nodegroups.get('nodegroups', []):
            ng_info = eks.describe_nodegroup(
                clusterName=CLUSTER_NAME,
                nodegroupName=ng_name
            )

            current_size = ng_info['nodegroup']['scalingConfig']['desiredSize']
            min_size = ng_info['nodegroup']['scalingConfig']['minSize']
            max_size = ng_info['nodegroup']['scalingConfig']['maxSize']

            # í˜„ì¬ í¬ê¸°ê°€ ìµœì†Œê°’ê³¼ ê°™ìœ¼ë©´ ì¦ê°€
            if current_size <= min_size and current_size < max_size:
                new_size = min(current_size + 1, max_size)

                eks.update_nodegroup_config(
                    clusterName=CLUSTER_NAME,
                    nodegroupName=ng_name,
                    scalingConfig={
                        'minSize': min_size,
                        'maxSize': max_size,
                        'desiredSize': new_size
                    }
                )

                result['details'] += f"Scaled nodegroup {ng_name} from {current_size} to {new_size}. "
                logger.info(result['details'])

        if not result['details']:
            result['details'] = "Node groups are already at or above minimum capacity."

    except Exception as e:
        result['success'] = False
        result['details'] = f"Failed to scale up nodes: {str(e)}"

    return result


def handle_resource_pressure(alarm_data: dict) -> dict:
    """
    ë¦¬ì†ŒìŠ¤ ì••ë°•(CPU/ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜) ì‹œì˜ ë³µêµ¬
    - ì•Œë¦¼ ë° ìŠ¤ì¼€ì¼ ì•„ì›ƒ ê¶Œì¥
    """
    result = {
        'alarm_name': alarm_data.get('AlarmName', ''),
        'action_taken': 'notification_with_recommendation',
        'success': True,
        'details': 'High resource utilization detected. Consider scaling out your application or nodes. '
                   'Check HPA settings if available.'
    }

    logger.info(result['details'])
    return result


def handle_unhealthy_hosts(alarm_data: dict) -> dict:
    """
    ë¹„ì •ìƒ í˜¸ìŠ¤íŠ¸ ê°ì§€ ì‹œì˜ ë³µêµ¬
    - ë¡œë“œë°¸ëŸ°ì„œì—ì„œ ë¹„ì •ìƒ íƒ€ê²Ÿ í™•ì¸ ë° ì•Œë¦¼
    """
    result = {
        'alarm_name': alarm_data.get('AlarmName', ''),
        'action_taken': 'notification_with_investigation',
        'success': True,
        'details': 'Unhealthy targets detected in load balancer. '
                   'Kubernetes readiness probes should handle pod-level issues. '
                   'Check pod logs and node status for investigation.'
    }

    logger.info(result['details'])
    return result


def send_notification(alarm_name: str, recovery_result: dict):
    """
    ë³µêµ¬ ì‘ì—… ê²°ê³¼ë¥¼ SNSë¡œ ì „ì†¡
    """
    if not SNS_TOPIC_ARN:
        logger.warning("SNS_TOPIC_ARN not set. Skipping notification.")
        return

    status = "SUCCESS" if recovery_result['success'] else "FAILED"

    message = f"""
ğŸ”§ EKS Auto Recovery Report

Environment: {ENVIRONMENT}
Cluster: {CLUSTER_NAME}
Alarm: {alarm_name}

Action Taken: {recovery_result['action_taken']}
Status: {status}

Details:
{recovery_result['details']}

---
This is an automated message from EKS Auto Recovery Lambda.
"""

    try:
        sns.publish(
            TopicArn=SNS_TOPIC_ARN,
            Subject=f"[{status}] EKS Auto Recovery - {alarm_name}",
            Message=message
        )
        logger.info(f"Notification sent for alarm: {alarm_name}")
    except Exception as e:
        logger.error(f"Failed to send notification: {str(e)}")
